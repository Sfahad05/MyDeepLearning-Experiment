{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87223845-72c7-4d03-a9b7-ac31758ac8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b7e403-9408-4d91-a9b5-d6870119b62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes (fixed order):\n",
      "0 ‚Üí self direction\n",
      "1 ‚Üí stimulation\n",
      "2 ‚Üí hedonism\n",
      "3 ‚Üí achievement\n",
      "4 ‚Üí power\n",
      "5 ‚Üí security\n",
      "6 ‚Üí conformity\n",
      "7 ‚Üí tradition\n",
      "8 ‚Üí benevolence\n",
      "9 ‚Üí universalism\n",
      "\n",
      "‚úÖ Final Splits: Train (85%): 9469, Test (15%): 1671\n",
      "\n",
      "üîç Grid Search + 3-Fold CV on TRAIN (85%)...\n",
      "  ‚Üí Trying: lr=1e-05, bs=8, ep=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚Üí CV Acc: 0.7356 ¬± 0.0015\n",
      "  ‚Üí Trying: lr=1e-05, bs=8, ep=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚Üí CV Acc: 0.7375 ¬± 0.0004\n",
      "  ‚Üí Trying: lr=1e-05, bs=16, ep=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚Üí CV Acc: 0.7261 ¬± 0.0075\n",
      "  ‚Üí Trying: lr=1e-05, bs=16, ep=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚Üí CV Acc: 0.7288 ¬± 0.0022\n",
      "  ‚Üí Trying: lr=2e-05, bs=8, ep=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚Üí CV Acc: 0.7434 ¬± 0.0053\n",
      "  ‚Üí Trying: lr=2e-05, bs=8, ep=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚Üí CV Acc: 0.7466 ¬± 0.0042\n",
      "  ‚Üí Trying: lr=2e-05, bs=16, ep=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚Üí CV Acc: 0.7355 ¬± 0.0050\n",
      "  ‚Üí Trying: lr=2e-05, bs=16, ep=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚Üí CV Acc: 0.7473 ¬± 0.0035\n",
      "  ‚Üí Trying: lr=3e-05, bs=8, ep=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚Üí CV Acc: 0.7477 ¬± 0.0025\n",
      "  ‚Üí Trying: lr=3e-05, bs=8, ep=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DistilBERT ‚Äî Leakage-Free Pipeline (Aligned with BiGRU/CNN)\n",
    "# 1. Split: Train (85%) + Test (15%) ‚Üê test frozen\n",
    "# 2. Grid search via 3-Fold CV on Train (85%)\n",
    "# 3. Final retrain on full oversampled Train (85%)\n",
    "# 4. Evaluate once on Test (15%)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler  # ‚úÖ Added for consistency\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# --- Reproducibility ---\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# --- Load Data ---\n",
    "df = pd.read_csv('newextendeddataset.csv', encoding='utf-8-sig')\n",
    "\n",
    "# --- Class Mapping (unchanged) ---\n",
    "y_dict = {\n",
    "    'self direction': 0, 'stimulation': 1, 'hedonism': 2, 'achievement': 3, 'power': 4,\n",
    "    'security': 5, 'conformity': 6, 'tradition': 7, 'benevolence': 8, 'universalism': 9\n",
    "}\n",
    "class_labels = [k for k in y_dict.keys()]\n",
    "n_classes = len(class_labels)\n",
    "\n",
    "df['category'] = df['category'].str.strip().str.lower()\n",
    "df = df[df['category'].isin(y_dict)]\n",
    "df['label_id'] = df['category'].map(y_dict).astype(int)\n",
    "\n",
    "print(\"Classes (fixed order):\")\n",
    "for i, name in enumerate(class_labels):\n",
    "    print(f\"{i} ‚Üí {name}\")\n",
    "\n",
    "# --- üîë REVISED SPLIT: 85% Train / 15% Test (NO VAL SET) ---\n",
    "texts = df['Base_Reviews'].values\n",
    "y_int = df['label_id'].values\n",
    "\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test = train_test_split(\n",
    "    texts, y_int, test_size=0.15, random_state=SEED, stratify=y_int\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Final Splits: Train (85%): {len(X_train_raw)}, Test (15%): {len(X_test_raw)}\")\n",
    "\n",
    "# --- Tokenizer (unchanged ‚Äî static, no leakage risk) ---\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize_texts(texts, max_len):\n",
    "    return tokenizer(\n",
    "        [str(t) for t in texts],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors='pt',\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "# --- Dataset wrapper (unchanged) ---\n",
    "class TextDS(Dataset):\n",
    "    def __init__(self, toks, labels):\n",
    "        self.ids = toks['input_ids']\n",
    "        self.msk = toks['attention_mask']\n",
    "        self.y = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            'input_ids': self.ids[i],\n",
    "            'attention_mask': self.msk[i],\n",
    "            'labels': self.y[i]\n",
    "        }\n",
    "\n",
    "# --- Train/Eval utilities (unchanged) ---\n",
    "def train_one_run(train_loader, val_loader, epochs, lr, weight_decay=0.01):\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased', num_labels=n_classes\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_val_acc = -1.0\n",
    "    best_state = None\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        tr_loss, tr_correct, tr_seen = 0.0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            masks = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(inputs, attention_mask=masks, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            tr_correct += (preds == labels).sum().item()\n",
    "            tr_seen += labels.size(0)\n",
    "\n",
    "        avg_tr_loss = tr_loss / len(train_loader)\n",
    "        avg_tr_acc = tr_correct / max(1, tr_seen)\n",
    "        history['train_loss'].append(avg_tr_loss)\n",
    "        history['train_acc'].append(avg_tr_acc)\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_seen = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = batch['input_ids'].to(device)\n",
    "                masks = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(inputs, attention_mask=masks, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_seen += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_acc = val_correct / max(1, val_seen)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(avg_val_acc)\n",
    "\n",
    "        if avg_val_acc > best_val_acc + 1e-6:\n",
    "            best_val_acc = avg_val_acc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "    return model, best_val_acc, history\n",
    "\n",
    "def evaluate_on_loader(model, data_loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            masks = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(inputs, attention_mask=masks, labels=labels)\n",
    "            total_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return acc, avg_loss, y_true, y_pred\n",
    "\n",
    "# =========================\n",
    "# GRID SEARCH + 3-FOLD CV ON TRAIN (85%)\n",
    "# =========================\n",
    "print(f\"\\nüîç Grid Search + 3-Fold CV on TRAIN (85%)...\")\n",
    "\n",
    "LR_LIST = [1e-5, 2e-5, 3e-5]\n",
    "BATCH_LIST = [8, 16]\n",
    "EPOCHS_LIST = [6, 8]\n",
    "MAXLEN = 128\n",
    "\n",
    "grid_results = []\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "\n",
    "for lr in LR_LIST:\n",
    "    for batch_size in BATCH_LIST:\n",
    "        for epochs in EPOCHS_LIST:\n",
    "            print(f\"  ‚Üí Trying: lr={lr}, bs={batch_size}, ep={epochs}\")\n",
    "            cv_scores = []\n",
    "            \n",
    "            for fold, (tr_idx, val_idx) in enumerate(skf.split(X_train_raw, y_train_raw), 1):\n",
    "                # Split\n",
    "                X_tr_raw = X_train_raw[tr_idx]\n",
    "                X_v_raw = X_train_raw[val_idx]\n",
    "                y_tr_raw = y_train_raw[tr_idx]\n",
    "                y_v_raw = y_train_raw[val_idx]\n",
    "                \n",
    "                # ‚úÖ Oversample ONLY fold-train\n",
    "                ros = RandomOverSampler(random_state=SEED)\n",
    "                X_tr_res, y_tr_res = ros.fit_resample(X_tr_raw.reshape(-1, 1), y_tr_raw)\n",
    "                X_tr_res = X_tr_res.flatten()  # back to list of strings\n",
    "                \n",
    "                # Tokenize (no fitting ‚Äî DistilBERT tokenizer is static)\n",
    "                tok_tr = tokenize_texts(X_tr_res, MAXLEN)  # oversampled!\n",
    "                tok_v = tokenize_texts(X_v_raw, MAXLEN)    # original val\n",
    "                \n",
    "                train_ds = TextDS(tok_tr, y_tr_res)\n",
    "                val_ds = TextDS(tok_v, y_v_raw)\n",
    "                \n",
    "                train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "                val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "                \n",
    "                # Train (fixed epochs for fairness)\n",
    "                model, val_acc, _ = train_one_run(\n",
    "                    train_loader, val_loader, epochs=epochs, lr=lr\n",
    "                )\n",
    "                cv_scores.append(val_acc)\n",
    "            \n",
    "            mean_cv = np.mean(cv_scores)\n",
    "            std_cv = np.std(cv_scores)\n",
    "            print(f\"    ‚Üí CV Acc: {mean_cv:.4f} ¬± {std_cv:.4f}\")\n",
    "            \n",
    "            grid_results.append({\n",
    "                'lr': lr, 'batch_size': batch_size, 'epochs': epochs,\n",
    "                'cv_mean_acc': mean_cv, 'cv_std_acc': std_cv\n",
    "            })\n",
    "\n",
    "# Select best by mean CV accuracy\n",
    "grid_df = pd.DataFrame(grid_results).sort_values('cv_mean_acc', ascending=False)\n",
    "best = grid_df.iloc[0]\n",
    "print(f\"\\n‚úÖ Best HPs by 3-Fold CV Acc ({best['cv_mean_acc']:.4f} ¬± {best['cv_std_acc']:.4f}):\")\n",
    "print({k: v for k, v in best.items() if k not in ['cv_mean_acc', 'cv_std_acc']})\n",
    "\n",
    "# =========================\n",
    "# 5-FOLD CV ON TRAIN (85%) WITH BEST HPs (Final Estimate)\n",
    "# =========================\n",
    "print(f\"\\nüöÄ 5-Fold CV on TRAIN (85%) with best HPs...\")\n",
    "\n",
    "skf_final = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "cv_results = []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf_final.split(X_train_raw, y_train_raw), 1):\n",
    "    print(f\"\\n--- Fold {fold}/5 ---\")\n",
    "    \n",
    "    X_tr_raw = X_train_raw[tr_idx]\n",
    "    X_v_raw = X_train_raw[val_idx]\n",
    "    y_tr_raw = y_train_raw[tr_idx]\n",
    "    y_v_raw = y_train_raw[val_idx]\n",
    "    \n",
    "    # ‚úÖ Oversample ONLY fold-train\n",
    "    ros = RandomOverSampler(random_state=SEED)\n",
    "    X_tr_res, y_tr_res = ros.fit_resample(X_tr_raw.reshape(-1, 1), y_tr_raw)\n",
    "    X_tr_res = X_tr_res.flatten()\n",
    "    \n",
    "    tok_tr = tokenize_texts(X_tr_res, MAXLEN)\n",
    "    tok_v = tokenize_texts(X_v_raw, MAXLEN)\n",
    "    \n",
    "    train_ds = TextDS(tok_tr, y_tr_res)\n",
    "    val_ds = TextDS(tok_v, y_v_raw)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=int(best['batch_size']), shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=int(best['batch_size']))\n",
    "    \n",
    "    model, val_acc, _ = train_one_run(\n",
    "        train_loader, val_loader, \n",
    "        epochs=int(best['epochs']), \n",
    "        lr=float(best['lr'])\n",
    "    )\n",
    "    print(f\"  ‚Üí Fold {fold} Val Acc: {val_acc:.4f}\")\n",
    "    cv_results.append({'fold': fold, 'val_acc': val_acc})\n",
    "\n",
    "# Summarize CV\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "mean_cv = cv_df['val_acc'].mean()\n",
    "std_cv = cv_df['val_acc'].std()\n",
    "print(f\"\\nüìä 5-Fold CV on TRAIN (85%): {mean_cv:.4f} ¬± {std_cv:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# FINAL MODEL: Retrain on FULL OVERSAMPLED TRAIN (85%)\n",
    "# =========================\n",
    "print(f\"\\nüéØ Retraining final DistilBERT on FULL TRAIN (85%)...\")\n",
    "\n",
    "# ‚úÖ Oversample FULL TRAIN only\n",
    "ros_final = RandomOverSampler(random_state=SEED)\n",
    "X_train_res, y_train_res = ros_final.fit_resample(X_train_raw.reshape(-1, 1), y_train_raw)\n",
    "X_train_res = X_train_res.flatten()\n",
    "\n",
    "# Tokenize\n",
    "tok_train_final = tokenize_texts(X_train_res, MAXLEN)\n",
    "tok_test_final = tokenize_texts(X_test_raw, MAXLEN)\n",
    "\n",
    "train_ds_final = TextDS(tok_train_final, y_train_res)\n",
    "test_ds_final = TextDS(tok_test_final, y_test)\n",
    "\n",
    "train_loader_final = DataLoader(train_ds_final, batch_size=int(best['batch_size']), shuffle=True)\n",
    "test_loader_final = DataLoader(test_ds_final, batch_size=int(best['batch_size']))\n",
    "\n",
    "# Train final model\n",
    "final_model, _, final_history = train_one_run(\n",
    "    train_loader_final, \n",
    "    DataLoader(TextDS(tokenize_texts(X_train_raw[:500], MAXLEN), y_train_raw[:500]), \n",
    "               batch_size=int(best['batch_size'])),  # small internal val for early stopping\n",
    "    epochs=int(best['epochs']), \n",
    "    lr=float(best['lr'])\n",
    ")\n",
    "\n",
    "# Save\n",
    "SAVE_DIR = './distilbert_best_cv'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "final_model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "print(f\"\\n‚úÖ Saved best model + tokenizer to: {SAVE_DIR}\")\n",
    "\n",
    "# =========================\n",
    "# FINAL EVALUATION ON TEST SET (15%)\n",
    "# =========================\n",
    "test_acc, test_loss, y_true_test, y_pred_test = evaluate_on_loader(final_model, test_loader_final)\n",
    "print(f\"\\nüü© FINAL TEST ACCURACY (Held-Out, Natural Distribution): {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\n=== FINAL CLASSIFICATION REPORT (TEST SET) ===\")\n",
    "print(classification_report(y_true_test, y_pred_test, target_names=class_labels, digits=4))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true_test, y_pred_test)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('DistilBERT (10-Class) Confusion Matrix ‚Äî Final Test')\n",
    "plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig('DistilBERT_10class_ConfusionMatrix_Test.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Learning Curves\n",
    "plt.figure(figsize=(13, 5))\n",
    "epochs = range(1, len(final_history['train_loss']) + 1)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, final_history['train_loss'], 'b-o', label='Train Loss')\n",
    "plt.plot(epochs, final_history['val_loss'], 'r-s', label='Val Loss')\n",
    "plt.title('Loss'); plt.xlabel('Epoch'); plt.legend(); plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, final_history['train_acc'], 'b-o', label='Train Acc')\n",
    "plt.plot(epochs, final_history['val_acc'], 'r-s', label='Val Acc')\n",
    "plt.title('Accuracy'); plt.xlabel('Epoch'); plt.legend(); plt.grid(True)\n",
    "\n",
    "plt.suptitle('DistilBERT Final Training')\n",
    "plt.tight_layout()\n",
    "plt.savefig('DistilBERT_10class_LearningCurves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019e55c9-7cd9-4e4b-abe7-41b664e10767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BERT (base-uncased) ‚Äî Leakage-Free Pipeline (Aligned with BiGRU/CNN/DistilBERT)\n",
    "# 1. Split: Train (85%) + Test (15%) ‚Üê test frozen\n",
    "# 2. Grid search via 3-Fold CV on Train (85%)\n",
    "# 3. Final retrain on full oversampled Train (85%)\n",
    "# 4. Evaluate once on Test (15%)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler  # ‚úÖ Added for consistency\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# --- Reproducibility ---\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# --- Load & Map Labels (unchanged) ---\n",
    "df = pd.read_csv('newextendeddataset.csv', encoding='utf-8-sig')\n",
    "\n",
    "y_dict = {\n",
    "    'self direction': 0, 'stimulation': 1, 'hedonism': 2, 'achievement': 3, 'power': 4,\n",
    "    'security': 5, 'conformity': 6, 'tradition': 7, 'benevolence': 8, 'universalism': 9\n",
    "}\n",
    "class_labels = [k for k in y_dict.keys()]\n",
    "n_classes = len(class_labels)\n",
    "\n",
    "df['category'] = df['category'].str.strip().str.lower()\n",
    "df = df[df['category'].isin(y_dict)]\n",
    "df['label_id'] = df['category'].map(y_dict).astype(int)\n",
    "\n",
    "# --- üîë REVISED SPLIT: 85% Train / 15% Test (NO VAL SET) ---\n",
    "texts = df['Base_Reviews'].values\n",
    "y_int = df['label_id'].values\n",
    "\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test = train_test_split(\n",
    "    texts, y_int, test_size=0.15, random_state=SEED, stratify=y_int\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Final Splits: Train (85%): {len(X_train_raw)}, Test (15%): {len(X_test_raw)}\")\n",
    "\n",
    "# --- Tokenizer (unchanged ‚Äî static) ---\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_texts(texts, max_len):\n",
    "    return tokenizer(\n",
    "        [str(t) for t in texts],\n",
    "        padding='max_length', truncation=True, max_length=max_len,\n",
    "        return_tensors='pt', return_attention_mask=True\n",
    "    )\n",
    "\n",
    "# --- Dataset wrapper (unchanged) ---\n",
    "class TextDS(Dataset):\n",
    "    def __init__(self, toks, labels):\n",
    "        self.ids = toks['input_ids']\n",
    "        self.msk = toks['attention_mask']\n",
    "        self.y = torch.tensor(labels, dtype=torch.long)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            'input_ids': self.ids[i],\n",
    "            'attention_mask': self.msk[i],\n",
    "            'labels': self.y[i]\n",
    "        }\n",
    "\n",
    "# --- Train/Eval utilities (unchanged) ---\n",
    "def train_one_run(train_loader, val_loader, epochs, lr, weight_decay=0.01):\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased', num_labels=n_classes\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_val_acc = -1.0\n",
    "    best_state = None\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        tr_loss, tr_correct, tr_seen = 0.0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            masks = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(inputs, attention_mask=masks, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            tr_correct += (preds == labels).sum().item()\n",
    "            tr_seen += labels.size(0)\n",
    "\n",
    "        avg_tr_loss = tr_loss / len(train_loader)\n",
    "        avg_tr_acc = tr_correct / max(1, tr_seen)\n",
    "        history['train_loss'].append(avg_tr_loss)\n",
    "        history['train_acc'].append(avg_tr_acc)\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_seen = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = batch['input_ids'].to(device)\n",
    "                masks = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(inputs, attention_mask=masks, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_seen += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_acc = val_correct / max(1, val_seen)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(avg_val_acc)\n",
    "\n",
    "        if avg_val_acc > best_val_acc + 1e-6:\n",
    "            best_val_acc = avg_val_acc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "    return model, best_val_acc, history\n",
    "\n",
    "def evaluate_on_loader(model, data_loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            masks = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(inputs, attention_mask=masks, labels=labels)\n",
    "            total_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return acc, avg_loss, y_true, y_pred\n",
    "\n",
    "# =========================\n",
    "# GRID SEARCH + 3-FOLD CV ON TRAIN (85%)\n",
    "# =========================\n",
    "print(f\"\\nüîç Grid Search + 3-Fold CV on TRAIN (85%)...\")\n",
    "\n",
    "LR_LIST = [2e-5, 3e-5, 5e-5]\n",
    "BATCH_LIST = [8, 16]\n",
    "EPOCHS_LIST = [6]\n",
    "MAXLEN = 128\n",
    "\n",
    "grid_results = []\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "\n",
    "for lr in LR_LIST:\n",
    "    for batch_size in BATCH_LIST:\n",
    "        for epochs in EPOCHS_LIST:\n",
    "            print(f\"  ‚Üí Trying: lr={lr}, bs={batch_size}, ep={epochs}\")\n",
    "            cv_scores = []\n",
    "            \n",
    "            for fold, (tr_idx, val_idx) in enumerate(skf.split(X_train_raw, y_train_raw), 1):\n",
    "                # Split\n",
    "                X_tr_raw = X_train_raw[tr_idx]\n",
    "                X_v_raw = X_train_raw[val_idx]\n",
    "                y_tr_raw = y_train_raw[tr_idx]\n",
    "                y_v_raw = y_train_raw[val_idx]\n",
    "                \n",
    "                # ‚úÖ Oversample ONLY fold-train\n",
    "                ros = RandomOverSampler(random_state=SEED)\n",
    "                X_tr_res, y_tr_res = ros.fit_resample(X_tr_raw.reshape(-1, 1), y_tr_raw)\n",
    "                X_tr_res = X_tr_res.flatten()\n",
    "                \n",
    "                # Tokenize (no fitting ‚Äî BERT tokenizer is static)\n",
    "                tok_tr = tokenize_texts(X_tr_res, MAXLEN)  # oversampled!\n",
    "                tok_v = tokenize_texts(X_v_raw, MAXLEN)    # original val\n",
    "                \n",
    "                train_ds = TextDS(tok_tr, y_tr_res)\n",
    "                val_ds = TextDS(tok_v, y_v_raw)\n",
    "                \n",
    "                train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "                val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "                \n",
    "                # Train (fixed epochs for fairness)\n",
    "                model, val_acc, _ = train_one_run(\n",
    "                    train_loader, val_loader, epochs=epochs, lr=lr\n",
    "                )\n",
    "                cv_scores.append(val_acc)\n",
    "            \n",
    "            mean_cv = np.mean(cv_scores)\n",
    "            std_cv = np.std(cv_scores)\n",
    "            print(f\"    ‚Üí CV Acc: {mean_cv:.4f} ¬± {std_cv:.4f}\")\n",
    "            \n",
    "            grid_results.append({\n",
    "                'lr': lr, 'batch_size': batch_size, 'epochs': epochs,\n",
    "                'cv_mean_acc': mean_cv, 'cv_std_acc': std_cv\n",
    "            })\n",
    "\n",
    "# Select best by mean CV accuracy\n",
    "grid_df = pd.DataFrame(grid_results).sort_values('cv_mean_acc', ascending=False)\n",
    "best = grid_df.iloc[0]\n",
    "print(f\"\\n‚úÖ Best HPs by 3-Fold CV Acc ({best['cv_mean_acc']:.4f} ¬± {best['cv_std_acc']:.4f}):\")\n",
    "print({k: v for k, v in best.items() if k not in ['cv_mean_acc', 'cv_std_acc']})\n",
    "\n",
    "# =========================\n",
    "# 5-FOLD CV ON TRAIN (85%) WITH BEST HPs (Final Estimate)\n",
    "# =========================\n",
    "print(f\"\\nüöÄ 5-Fold CV on TRAIN (85%) with best HPs...\")\n",
    "\n",
    "skf_final = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "cv_results = []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf_final.split(X_train_raw, y_train_raw), 1):\n",
    "    print(f\"\\n--- Fold {fold}/5 ---\")\n",
    "    \n",
    "    X_tr_raw = X_train_raw[tr_idx]\n",
    "    X_v_raw = X_train_raw[val_idx]\n",
    "    y_tr_raw = y_train_raw[tr_idx]\n",
    "    y_v_raw = y_train_raw[val_idx]\n",
    "    \n",
    "    # ‚úÖ Oversample ONLY fold-train\n",
    "    ros = RandomOverSampler(random_state=SEED)\n",
    "    X_tr_res, y_tr_res = ros.fit_resample(X_tr_raw.reshape(-1, 1), y_tr_raw)\n",
    "    X_tr_res = X_tr_res.flatten()\n",
    "    \n",
    "    tok_tr = tokenize_texts(X_tr_res, MAXLEN)\n",
    "    tok_v = tokenize_texts(X_v_raw, MAXLEN)\n",
    "    \n",
    "    train_ds = TextDS(tok_tr, y_tr_res)\n",
    "    val_ds = TextDS(tok_v, y_v_raw)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=int(best['batch_size']), shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=int(best['batch_size']))\n",
    "    \n",
    "    model, val_acc, _ = train_one_run(\n",
    "        train_loader, val_loader, \n",
    "        epochs=int(best['epochs']), \n",
    "        lr=float(best['lr'])\n",
    "    )\n",
    "    print(f\"  ‚Üí Fold {fold} Val Acc: {val_acc:.4f}\")\n",
    "    cv_results.append({'fold': fold, 'val_acc': val_acc})\n",
    "\n",
    "# Summarize CV\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "mean_cv = cv_df['val_acc'].mean()\n",
    "std_cv = cv_df['val_acc'].std()\n",
    "print(f\"\\nüìä 5-Fold CV on TRAIN (85%): {mean_cv:.4f} ¬± {std_cv:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# FINAL MODEL: Retrain on FULL OVERSAMPLED TRAIN (85%)\n",
    "# =========================\n",
    "print(f\"\\nüéØ Retraining final BERT on FULL TRAIN (85%)...\")\n",
    "\n",
    "# ‚úÖ Oversample FULL TRAIN only\n",
    "ros_final = RandomOverSampler(random_state=SEED)\n",
    "X_train_res, y_train_res = ros_final.fit_resample(X_train_raw.reshape(-1, 1), y_train_raw)\n",
    "X_train_res = X_train_res.flatten()\n",
    "\n",
    "# Tokenize\n",
    "tok_train_final = tokenize_texts(X_train_res, MAXLEN)\n",
    "tok_test_final = tokenize_texts(X_test_raw, MAXLEN)\n",
    "\n",
    "train_ds_final = TextDS(tok_train_final, y_train_res)\n",
    "test_ds_final = TextDS(tok_test_final, y_test)\n",
    "\n",
    "train_loader_final = DataLoader(train_ds_final, batch_size=int(best['batch_size']), shuffle=True)\n",
    "test_loader_final = DataLoader(test_ds_final, batch_size=int(best['batch_size']))\n",
    "\n",
    "# Train final model\n",
    "final_model, _, final_history = train_one_run(\n",
    "    train_loader_final, \n",
    "    DataLoader(TextDS(tokenize_texts(X_train_raw[:500], MAXLEN), y_train_raw[:500]), \n",
    "               batch_size=int(best['batch_size'])),  # small internal val for early stopping\n",
    "    epochs=int(best['epochs']), \n",
    "    lr=float(best['lr'])\n",
    ")\n",
    "\n",
    "# Save\n",
    "SAVE_DIR = './bert_best_cv'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "final_model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "print(f\"\\n‚úÖ Saved best BERT model + tokenizer to: {SAVE_DIR}\")\n",
    "\n",
    "# =========================\n",
    "# FINAL EVALUATION ON TEST SET (15%)\n",
    "# =========================\n",
    "test_acc, test_loss, y_true_test, y_pred_test = evaluate_on_loader(final_model, test_loader_final)\n",
    "print(f\"\\nüü© FINAL TEST ACCURACY (Held-Out, Natural Distribution): {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\n=== FINAL CLASSIFICATION REPORT (TEST SET) ===\")\n",
    "print(classification_report(y_true_test, y_pred_test, target_names=class_labels, digits=4))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true_test, y_pred_test)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('BERT (10-Class) Confusion Matrix ‚Äî Final Test')\n",
    "plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig('BERT_10class_ConfusionMatrix_Test.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Learning Curves\n",
    "plt.figure(figsize=(13, 5))\n",
    "epochs = range(1, len(final_history['train_loss']) + 1)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, final_history['train_loss'], 'b-o', label='Train Loss')\n",
    "plt.plot(epochs, final_history['val_loss'], 'r-s', label='Val Loss')\n",
    "plt.title('Loss'); plt.xlabel('Epoch'); plt.legend(); plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, final_history['train_acc'], 'b-o', label='Train Acc')\n",
    "plt.plot(epochs, final_history['val_acc'], 'r-s', label='Val Acc')\n",
    "plt.title('Accuracy'); plt.xlabel('Epoch'); plt.legend(); plt.grid(True)\n",
    "\n",
    "plt.suptitle('BERT Final Training')\n",
    "plt.tight_layout()\n",
    "plt.savefig('BERT_10class_LearningCurves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
